{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'feedparser'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfeedparser\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'feedparser'"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def scrape_google_news(topic, limit):\n",
    "    base_url = \"https://news.google.com/rss\"\n",
    "    results = []\n",
    "    page = 1\n",
    "    \n",
    "    while len(results) < limit:\n",
    "        # Construct the URL for the Google News RSS feed\n",
    "        url = f\"{base_url}/search?q={topic}&hl=en-US&gl=US&ceid=US:en\"\n",
    "        if page > 1:\n",
    "            url += f\"&page={page}\"\n",
    "        \n",
    "        # Parse the RSS feed\n",
    "        feed = feedparser.parse(url)\n",
    "        \n",
    "        # Check if there are no more articles\n",
    "        if len(feed.entries) == 0:\n",
    "            break\n",
    "        \n",
    "        for article in feed.entries:\n",
    "            if len(results) >= limit:\n",
    "                break\n",
    "            results.append({\n",
    "                'title': article.title,\n",
    "                'link': article.link,\n",
    "                'published': article.published,\n",
    "                'summary': article.summary if 'summary' in article else ''\n",
    "            })\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(1)  # Add a delay to avoid overwhelming the server\n",
    "    \n",
    "    return results[:limit]\n",
    "\n",
    "# Example usage\n",
    "topic = \"artificial intelligence\"\n",
    "limit = 200\n",
    "\n",
    "news_articles = scrape_google_news(topic, limit)\n",
    "\n",
    "print(f\"Total articles fetched: {len(news_articles)}\")\n",
    "\n",
    "for i, article in enumerate(news_articles, 1):\n",
    "    print(f\"{i}. Title: {article['title']}\")\n",
    "    print(f\"   Link: {article['link']}\")\n",
    "    print(f\"   Published: {article['published']}\")\n",
    "    print(f\"   Summary: {article['summary'][:100]}...\")  # Truncate summary for brevity\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select a time range:\n",
      "1. Recent past hours\n",
      "2. Last 24 hours\n",
      "3. Past week\n",
      "4. Last month\n",
      "5. Last year\n",
      "6. Custom date range\n",
      "\n",
      "Here are the URLs to scrape 500 news titles:\n",
      "\n",
      "URL 1:\n",
      "https://www.google.com/search?q=gamese&tbm=nws&num=100&start=0&tbs=qdr%3Aw\n",
      "\n",
      "URL 2:\n",
      "https://www.google.com/search?q=gamese&tbm=nws&num=100&start=100&tbs=qdr%3Aw\n",
      "\n",
      "URL 3:\n",
      "https://www.google.com/search?q=gamese&tbm=nws&num=100&start=200&tbs=qdr%3Aw\n",
      "\n",
      "URL 4:\n",
      "https://www.google.com/search?q=gamese&tbm=nws&num=100&start=300&tbs=qdr%3Aw\n",
      "\n",
      "URL 5:\n",
      "https://www.google.com/search?q=gamese&tbm=nws&num=100&start=400&tbs=qdr%3Aw\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "from math import ceil\n",
    "\n",
    "def get_time_range():\n",
    "    print(\"\\nSelect a time range:\")\n",
    "    print(\"1. Recent past hours\")\n",
    "    print(\"2. Last 24 hours\")\n",
    "    print(\"3. Past week\")\n",
    "    print(\"4. Last month\")\n",
    "    print(\"5. Last year\")\n",
    "    print(\"6. Custom date range\")\n",
    "    \n",
    "    choice = input(\"Enter your choice (1-6): \")\n",
    "    \n",
    "    if choice == '1':\n",
    "        hours = int(input(\"Enter the number of past hours: \"))\n",
    "        return f\"qdr:h{hours}\"\n",
    "    elif choice == '2':\n",
    "        return \"qdr:d\"\n",
    "    elif choice == '3':\n",
    "        return \"qdr:w\"\n",
    "    elif choice == '4':\n",
    "        return \"qdr:m\"\n",
    "    elif choice == '5':\n",
    "        return \"qdr:y\"\n",
    "    elif choice == '6':\n",
    "        start_date = input(\"Enter start date (MM/DD/YYYY): \")\n",
    "        end_date = input(\"Enter end date (MM/DD/YYYY): \")\n",
    "        return f\"cdr:1,cd_min:{start_date},cd_max:{end_date}\"\n",
    "    else:\n",
    "        print(\"Invalid choice. Using default (no time range).\")\n",
    "        return \"\"\n",
    "\n",
    "def generate_urls(query, time_range, limit):\n",
    "    base_url = \"https://www.google.com/search\"\n",
    "    urls = []\n",
    "    results_per_page = 100\n",
    "    num_pages = ceil(limit / results_per_page)\n",
    "    \n",
    "    for page in range(num_pages):\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"tbm\": \"nws\",\n",
    "            \"num\": min(results_per_page, limit - page * results_per_page),\n",
    "            \"start\": page * results_per_page\n",
    "        }\n",
    "        \n",
    "        if time_range:\n",
    "            params[\"tbs\"] = time_range\n",
    "        \n",
    "        url = f\"{base_url}?{urllib.parse.urlencode(params)}\"\n",
    "        urls.append(url)\n",
    "    \n",
    "    return urls\n",
    "\n",
    "def main():\n",
    "    query = input(\"Enter your search query: \")\n",
    "    time_range = get_time_range()\n",
    "    limit = int(input(\"Enter the number of news titles to scrape (max 1000): \"))\n",
    "    \n",
    "    if limit > 1000:\n",
    "        print(\"Limiting to 1000 results as per Google's typical maximum.\")\n",
    "        limit = 1000\n",
    "    \n",
    "    urls = generate_urls(query, time_range, limit)\n",
    "    \n",
    "    print(f\"\\nHere are the URLs to scrape {limit} news titles:\")\n",
    "    for i, url in enumerate(urls, 1):\n",
    "        print(f\"\\nURL {i}:\")\n",
    "        print(url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select a time range:\n",
      "1. Recent past hours\n",
      "2. Last 24 hours\n",
      "3. Past week\n",
      "4. Last month\n",
      "5. Last year\n",
      "6. Custom date range\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from math import ceil\n",
    "from requests_html import HTMLSession\n",
    "def get_time_range():\n",
    "    print(\"\\nSelect a time range:\")\n",
    "    print(\"1. Recent past hours\")\n",
    "    print(\"2. Last 24 hours\")\n",
    "    print(\"3. Past week\")\n",
    "    print(\"4. Last month\")\n",
    "    print(\"5. Last year\")\n",
    "    print(\"6. Custom date range\")\n",
    "    \n",
    "    choice = input(\"Enter your choice (1-6): \")\n",
    "    \n",
    "    if choice == '1':\n",
    "        hours = int(input(\"Enter the number of past hours: \"))\n",
    "        return f\"qdr:h{hours}\"\n",
    "    elif choice == '2':\n",
    "        return \"qdr:d\"\n",
    "    elif choice == '3':\n",
    "        return \"qdr:w\"\n",
    "    elif choice == '4':\n",
    "        return \"qdr:m\"\n",
    "    elif choice == '5':\n",
    "        return \"qdr:y\"\n",
    "    elif choice == '6':\n",
    "        start_date = input(\"Enter start date (MM/DD/YYYY): \")\n",
    "        end_date = input(\"Enter end date (MM/DD/YYYY): \")\n",
    "        return f\"cdr:1,cd_min:{start_date},cd_max:{end_date}\"\n",
    "    else:\n",
    "        print(\"Invalid choice. Using default (no time range).\")\n",
    "        return \"\"\n",
    "\n",
    "def get_total_results(query, time_range):\n",
    "    base_url = \"https://www.google.com/search\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"tbm\": \"nws\",\n",
    "        \"num\": 1\n",
    "    }\n",
    "    if time_range:\n",
    "        params[\"tbs\"] = time_range\n",
    "    \n",
    "    url = f\"{base_url}?{urllib.parse.urlencode(params)}\"\n",
    "    \n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        r = session.get(url)\n",
    "        r.html.render(sleep=4)\n",
    "        soup = BeautifulSoup(r.html.raw_html, \"html.parser\")\n",
    "        print(soup)\n",
    "        result_stats = soup.find('div', {'id': 'result-stats'})\n",
    "        if result_stats:\n",
    "            total_results = int(''.join(filter(str.isdigit, result_stats.text)))\n",
    "            print(total_results)\n",
    "            return min(total_results, 1000)  # Google typically limits to 1000 results\n",
    "        else:\n",
    "            print(\"Couldn't find total results. Defaulting to 1000.\")\n",
    "            return 1000\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching total results: {e}\")\n",
    "        return 1000\n",
    "\n",
    "def generate_urls(query, time_range, limit):\n",
    "    total_results = get_total_results(query, time_range)\n",
    "    limit = min(limit, total_results)\n",
    "    \n",
    "    base_url = \"https://www.google.com/search\"\n",
    "    urls = []\n",
    "    results_per_page = 100\n",
    "    num_pages = ceil(limit / results_per_page)\n",
    "    \n",
    "    for page in range(num_pages):\n",
    "        remaining = limit - page * results_per_page\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"tbm\": \"nws\",\n",
    "            \"num\": min(results_per_page, remaining),\n",
    "            \"start\": page * results_per_page\n",
    "        }\n",
    "        \n",
    "        if time_range:\n",
    "            params[\"tbs\"] = time_range\n",
    "        \n",
    "        url = f\"{base_url}?{urllib.parse.urlencode(params)}\"\n",
    "        urls.append(url)\n",
    "    \n",
    "    return urls, limit\n",
    "\n",
    "def main():\n",
    "    query = input(\"Enter your search query: \")\n",
    "    time_range = get_time_range()\n",
    "    limit = int(input(\"Enter the number of news titles to scrape: \"))\n",
    "    \n",
    "    urls, actual_limit = generate_urls(query, time_range, limit)\n",
    "    \n",
    "    print(f\"\\nURLs to scrape {actual_limit} news titles (or all available if less):\")\n",
    "    for i, url in enumerate(urls, 1):\n",
    "        print(f\"\\nURL {i}:\")\n",
    "        print(url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response saved to 'parsed_response.html'\n"
     ]
    }
   ],
   "source": [
    "from requests_html import HTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "session = HTMLSession()\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.google.com/search?q=games&tbm=nws&num=100&start=0&tbs=qdr%3Aw\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "r = session.get(url)\n",
    "r.html.render(sleep=4)  # Render the JavaScript content\n",
    "# Parse the response content with BeautifulSoup\n",
    "soup = BeautifulSoup(r.html.raw_html, \"html.parser\")\n",
    "\n",
    "# Save the parsed HTML to a file\n",
    "with open(\"parsed_response.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(soup.prettify())\n",
    "\n",
    "print(\"Parsed response saved to 'parsed_response.html'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
